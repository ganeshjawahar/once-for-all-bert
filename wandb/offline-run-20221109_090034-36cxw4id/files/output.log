Subnet info: Model-Size=49640928, Val-PPL=7.7669284905290885
Subnet info: Gene=[120, 120, 120, 240, 240, 360, 360, 360, 240, 360, 360, 360]
Subnet info: Search_space_id=
Subnet info: elastic_keys= ['sample_hidden_size']
Subnet info: gene_choices= [[120, 240, 360, 480, 540, 600, 768], [120, 240, 360, 480, 540, 600, 768], [120, 240, 360, 480, 540, 600, 768], [120, 240, 360, 480, 540, 600, 768], [120, 240, 360, 480, 540, 600, 768], [120, 240, 360, 480, 540, 600, 768], [120, 240, 360, 480, 540, 600, 768], [120, 240, 360, 480, 540, 600, 768], [120, 240, 360, 480, 540, 600, 768], [120, 240, 360, 480, 540, 600, 768], [120, 240, 360, 480, 540, 600, 768], [120, 240, 360, 480, 540, 600, 768]]
Subnet info: gene_names= ['sample_hidden_size_0', 'sample_hidden_size_1', 'sample_hidden_size_2', 'sample_hidden_size_3', 'sample_hidden_size_4', 'sample_hidden_size_5', 'sample_hidden_size_6', 'sample_hidden_size_7', 'sample_hidden_size_8', 'sample_hidden_size_9', 'sample_hidden_size_10', 'sample_hidden_size_11']
Subnet info: elastickey2ranges= {'sample_hidden_size': [0, 12]}
Number of parameters in custom config is 50 Million
=====================================================================================
Layer (type:depth-idx)                                       Param #
=====================================================================================
BertForSequenceClassification                                --
├─BertModel: 1-1                                             --
│    └─BertEmbeddings: 2-1                                   --
│    │    └─CustomEmbedding: 3-1                             23,440,896
│    │    └─CustomEmbedding: 3-2                             393,216
│    │    └─CustomEmbedding: 3-3                             1,536
│    │    └─CustomLayerNorm: 3-4                             1,536
│    │    └─Dropout: 3-5                                     --
│    └─BertEncoder: 2-2                                      --
│    │    └─ModuleList: 3-6                                  --
│    │    │    └─BertLayer: 4-1                              12,994,946
│    │    │    └─BertLayer: 4-2                              12,994,946
│    │    │    └─BertLayer: 4-3                              12,994,946
│    │    │    └─BertLayer: 4-4                              12,994,946
│    │    │    └─BertLayer: 4-5                              12,994,946
│    │    │    └─BertLayer: 4-6                              12,994,946
│    │    │    └─BertLayer: 4-7                              12,994,946
│    │    │    └─BertLayer: 4-8                              12,994,946
│    │    │    └─BertLayer: 4-9                              12,994,946
│    │    │    └─BertLayer: 4-10                             12,994,946
│    │    │    └─BertLayer: 4-11                             12,994,946
│    │    │    └─BertLayer: 4-12                             12,994,946
│    └─BertPooler: 2-3                                       --
│    │    └─CustomLinear: 3-7                                590,592
│    │    └─Tanh: 3-8                                        --
├─Dropout: 1-2                                               --
├─CustomLinear: 1-3                                          1,538
=====================================================================================
Total params: 180,368,666
Trainable params: 180,350,234
Non-trainable params: 18,432
=====================================================================================
setting teachers requires_grad to False
setting the subnet...
BertConfig {
  "_name_or_path": "/fsx/ganayu/experiments/supershaper/sep23_mnlikd_models_master/b828a1f8-a42d-4e66-b7be-eabeea7bf725/mnli_best",
  "add_distill_linear_layer": false,
  "additional_random_softmaxing": false,
  "alpha_divergence": 0,
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bottleneck_rank": 50,
  "classifier_dropout": null,
  "expert_routing_type": "archrouting_jack_2L",
  "fixed_hypernet_input": "no",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0,
  "hidden_size": 768,
  "hypernet_hidden_size": 128,
  "hypernet_input_format": "standard",
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "last_expert_averaging_expert": "no",
  "layer_drop_prob": 0.0,
  "layer_norm_eps": 1e-12,
  "max_experts": 2,
  "max_position_embeddings": 512,
  "max_seq_length": 128,
  "mixing": "bert-bottleneck",
  "model_type": "bert",
  "normalization_type": "layer_norm",
  "num_attention_heads": 12,
  "num_feedforward_networks": 1,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "random_layer_selection_probability": 0.1,
  "rewire": 0,
  "sample_expert_ids": [
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0
  ],
  "sample_hidden_size": [
    120,
    120,
    120,
    240,
    240,
    360,
    360,
    360,
    240,
    360,
    360,
    360
  ],
  "sample_intermediate_size": [
    3072,
    3072,
    3072,
    3072,
    3072,
    3072,
    3072,
    3072,
    3072,
    3072,
    3072,
    3072
  ],
  "sample_num_attention_heads": [
    12,
    12,
    12,
    12,
    12,
    12,
    12,
    12,
    12,
    12,
    12,
    12
  ],
  "sample_num_hidden_layers": 12,
  "search_space_id": null,
  "transformers_version": "4.11.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "use_hypernet_w_low_rank": 0,
  "vocab_size": 30522
}
{'accuracy': 0.4652777777777778}
{'SuperTransformer Val Accuracy': 0.4652777777777778, 'task': 'rte', 'epoch': 0, 'num_train_epochs': 4}
{'accuracy': 0.4652777777777778}
{'SuperTransformer Val Accuracy': 0.4652777777777778, 'task': 'rte', 'epoch': 1, 'num_train_epochs': 4}
{'accuracy': 0.4652777777777778}
{'SuperTransformer Val Accuracy': 0.4652777777777778, 'task': 'rte', 'epoch': 2, 'num_train_epochs': 4}
{'accuracy': 0.4652777777777778}
{'SuperTransformer Val Accuracy': 0.4652777777777778, 'task': 'rte', 'epoch': 3, 'num_train_epochs': 4}
loading configuration file /fsx/ganayu/experiments/supershaper/sep23_mnlikd_models_master/b828a1f8-a42d-4e66-b7be-eabeea7bf725/mnli_best/config.json
/data/home/ganayu/miniconda/envs/basic/lib/python3.8/site-packages/transformers/configuration_utils.py:336: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.
  warnings.warn(
Model config BertConfig {
  "_name_or_path": "/fsx/ganayu/experiments/supershaper/oct18_supernet_v1_moe_archrouting_jack_2L/archrouting_jack_2L_hyp128/best_model",
  "add_distill_linear_layer": false,
  "additional_random_softmaxing": false,
  "alpha_divergence": 0,
  "architectures": [
    "BertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bottleneck_rank": 50,
  "classifier_dropout": null,
  "expert_routing_type": "archrouting_jack_2L",
  "fixed_hypernet_input": "no",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0,
  "hidden_size": 768,
  "hypernet_hidden_size": 128,
  "hypernet_input_format": "standard",
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "last_expert_averaging_expert": "no",
  "layer_drop_prob": 0.0,
  "layer_norm_eps": 1e-12,
  "max_experts": 2,
  "max_position_embeddings": 512,
  "max_seq_length": 128,
  "mixing": "bert-bottleneck",
  "model_type": "bert",
  "normalization_type": "layer_norm",
  "num_attention_heads": 12,
  "num_feedforward_networks": 1,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "random_layer_selection_probability": 0.1,
  "rewire": 0,
  "sample_expert_ids": [
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0
  ],
  "sample_hidden_size": [
    120,
    120,
    120,
    240,
    240,
    360,
    360,
    360,
    240,
    360,
    360,
    360
  ],
  "sample_intermediate_size": [
    3072,
    3072,
    3072,
    3072,
    3072,
    3072,
    3072,
    3072,
    3072,
    3072,
    3072,
    3072
  ],
  "sample_num_attention_heads": [
    12,
    12,
    12,
    12,
    12,
    12,
    12,
    12,
    12,
    12,
    12,
    12
  ],
  "sample_num_hidden_layers": 12,
  "search_space_id": null,
  "torch_dtype": "float32",
  "transformers_version": "4.11.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "use_hypernet_w_low_rank": 0,
  "vocab_size": 30522
}
loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /data/home/ganayu/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.11.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}
loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /data/home/ganayu/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /data/home/ganayu/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /data/home/ganayu/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /data/home/ganayu/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.11.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}
loading configuration file /fsx/ganayu/experiments/supershaper/sep23_mnlikd_models_master/b828a1f8-a42d-4e66-b7be-eabeea7bf725/mnli_best/config.json
Model config BertConfig {
  "_name_or_path": "/fsx/ganayu/experiments/supershaper/oct18_supernet_v1_moe_archrouting_jack_2L/archrouting_jack_2L_hyp128/best_model",
  "add_distill_linear_layer": false,
  "additional_random_softmaxing": false,
  "alpha_divergence": 0,
  "architectures": [
    "BertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bottleneck_rank": 50,
  "classifier_dropout": null,
  "expert_routing_type": "archrouting_jack_2L",
  "finetuning_task": "rte",
  "fixed_hypernet_input": "no",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0,
  "hidden_size": 768,
  "hypernet_hidden_size": 128,
  "hypernet_input_format": "standard",
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "last_expert_averaging_expert": "no",
  "layer_drop_prob": 0.0,
  "layer_norm_eps": 1e-12,
  "max_experts": 2,
  "max_position_embeddings": 512,
  "max_seq_length": 128,
  "mixing": "bert-bottleneck",
  "model_type": "bert",
  "normalization_type": "layer_norm",
  "num_attention_heads": 12,
  "num_feedforward_networks": 1,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "random_layer_selection_probability": 0.1,
  "rewire": 0,
  "sample_expert_ids": [
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0
  ],
  "sample_hidden_size": [
    120,
    120,
    120,
    240,
    240,
    360,
    360,
    360,
    240,
    360,
    360,
    360
  ],
  "sample_intermediate_size": [
    3072,
    3072,
    3072,
    3072,
    3072,
    3072,
    3072,
    3072,
    3072,
    3072,
    3072,
    3072
  ],
  "sample_num_attention_heads": [
    12,
    12,
    12,
    12,
    12,
    12,
    12,
    12,
    12,
    12,
    12,
    12
  ],
  "sample_num_hidden_layers": 12,
  "search_space_id": null,
  "torch_dtype": "float32",
  "transformers_version": "4.11.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "use_hypernet_w_low_rank": 0,
  "vocab_size": 30522
}
loading weights file /fsx/ganayu/experiments/supershaper/sep23_mnlikd_models_master/b828a1f8-a42d-4e66-b7be-eabeea7bf725/mnli_best/pytorch_model.bin
All model checkpoint weights were used when initializing BertForSequenceClassification.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /fsx/ganayu/experiments/supershaper/sep23_mnlikd_models_master/b828a1f8-a42d-4e66-b7be-eabeea7bf725/mnli_best and are newly initialized because the shapes did not match:
- classifier.weight: found shape torch.Size([3, 768]) in the checkpoint and torch.Size([2, 768]) in the model instantiated
- classifier.bias: found shape torch.Size([3]) in the checkpoint and torch.Size([2]) in the model instantiated
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
loading configuration file /fsx/ganayu/experiments/supershaper/aug25_finetune_googlebert_mrpc_rte_stsb_ckptneeded/22_bertbase_rte_5e-5_32_3/config.json
Model config BertConfig {
  "_name_or_path": "/fsx/ganayu/experiments/supershaper/aug25_finetune_googlebert_mnli_cola_ckptneeded/6_bertbase_mnli_3e-5_16_2",
  "architectures": [
    "BertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "finetuning_task": "rte",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.11.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}
loading weights file /fsx/ganayu/experiments/supershaper/aug25_finetune_googlebert_mrpc_rte_stsb_ckptneeded/22_bertbase_rte_5e-5_32_3/pytorch_model.bin
All model checkpoint weights were used when initializing BertForSequenceClassification.
All the weights of BertForSequenceClassification were initialized from the model checkpoint at /fsx/ganayu/experiments/supershaper/aug25_finetune_googlebert_mrpc_rte_stsb_ckptneeded/22_bertbase_rte_5e-5_32_3.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.
11/09/2022 09:00:57 - WARNING - datasets.fingerprint -   Parameter 'function'=<function main.<locals>.preprocess_function at 0x7fdbbc7320d0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
11/09/2022 09:00:57 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at /data/home/ganayu/.cache/huggingface/datasets/glue/rte/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-59ce8cfd8e09b24c.arrow
11/09/2022 09:00:57 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at /data/home/ganayu/.cache/huggingface/datasets/glue/rte/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-373bf2215a6709ea.arrow
11/09/2022 09:00:57 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at /data/home/ganayu/.cache/huggingface/datasets/glue/rte/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-6786bf68fba9f210.arrow
11/09/2022 09:00:57 - INFO - __main__ -   Sample 336 of the training set: {'input_ids': [101, 2976, 3914, 3472, 1010, 5070, 15505, 9739, 1010, 7420, 5958, 2008, 3522, 12154, 1999, 1057, 1012, 1055, 1012, 2188, 7597, 1010, 4518, 5300, 1998, 2060, 3596, 1997, 7177, 2089, 2022, 5741, 1998, 2071, 4089, 9413, 10244, 2065, 2146, 1011, 2744, 3037, 6165, 4125, 1012, 102, 15505, 9739, 2036, 5191, 1999, 2010, 4613, 2055, 2054, 2097, 5258, 2007, 1996, 4566, 1997, 1996, 3522, 8760, 2558, 1997, 2659, 3037, 6165, 1998, 2659, 10831, 2005, 9387, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': 1}.
11/09/2022 09:00:57 - INFO - __main__ -   Sample 1156 of the training set: {'input_ids': [101, 7513, 1005, 1055, 1006, 5796, 6199, 1007, 2219, 3945, 3194, 2097, 8642, 5672, 1996, 20643, 1006, 1061, 6806, 2080, 1007, 3945, 2974, 2008, 2038, 2042, 2109, 2006, 7513, 1005, 1055, 5796, 2078, 4037, 1012, 102, 20643, 1005, 1055, 2974, 2097, 2145, 2022, 2109, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': 1}.
11/09/2022 09:00:57 - INFO - __main__ -   Sample 2202 of the training set: {'input_ids': [101, 2348, 2087, 21524, 4625, 2046, 5279, 1999, 2344, 2000, 4497, 1998, 2156, 9064, 1010, 9302, 4584, 2360, 2008, 1996, 3522, 3675, 8488, 9124, 20673, 25555, 2000, 3288, 5850, 1998, 4255, 2046, 14474, 1012, 102, 6811, 1998, 9302, 3675, 4932, 3478, 8385, 1999, 4073, 2000, 2203, 1996, 8488, 1998, 2000, 2485, 1996, 3675, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': 1}.
11/09/2022 09:01:06 - INFO - __main__ -   ***** Running training *****
11/09/2022 09:01:06 - INFO - __main__ -     Num examples = 2490
11/09/2022 09:01:06 - INFO - __main__ -     Num Epochs = 4
11/09/2022 09:01:06 - INFO - __main__ -     Instantaneous batch size per device = 8
11/09/2022 09:01:06 - INFO - __main__ -     Total train batch size (w. parallel, distributed & accumulation) = 16
11/09/2022 09:01:06 - INFO - __main__ -     Gradient Accumulation steps = 1
11/09/2022 09:01:06 - INFO - __main__ -     Total optimization steps = 624
  0%|                                                                                                             | 0/624 [00:00<?, ?it/s]11/09/2022 09:01:08 - INFO - root -   Reducer buckets have been rebuilt in this iteration.
11/09/2022 09:01:21 - INFO - __main__ -   Training completed. Find your checkpoints at /fsx/ganayu/experiments/supershaper/sep23_mnlikd_models_master/b828a1f8-a42d-4e66-b7be-eabeea7bf725
  0%|                                                                                                             | 0/624 [00:14<?, ?it/s]