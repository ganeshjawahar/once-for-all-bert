Subnet info: Model-Size=66958800, Val-PPL=5.571783852099099
Subnet info: Gene=[600, 2, 3, 2, 3, 3, 3, 3, 3, 3, 3, 4, 4, 12, 12, 6, 12, 12, 12, 12, 12, 12, 12, 12, 12]
Subnet info: Search_space_id=v3
Subnet info: elastic_keys= ['sample_hidden_size', 'sample_intermediate_size', 'sample_num_attention_heads']
Subnet info: gene_choices= [[120, 240, 360, 480, 540, 600, 768], [2, 3, 4], [2, 3, 4], [2, 3, 4], [2, 3, 4], [2, 3, 4], [2, 3, 4], [2, 3, 4], [2, 3, 4], [2, 3, 4], [2, 3, 4], [2, 3, 4], [2, 3, 4], [6, 12], [6, 12], [6, 12], [6, 12], [6, 12], [6, 12], [6, 12], [6, 12], [6, 12], [6, 12], [6, 12], [6, 12]]
Subnet info: gene_names= ['sample_hidden_size_0', 'sample_intermediate_size_0', 'sample_intermediate_size_1', 'sample_intermediate_size_2', 'sample_intermediate_size_3', 'sample_intermediate_size_4', 'sample_intermediate_size_5', 'sample_intermediate_size_6', 'sample_intermediate_size_7', 'sample_intermediate_size_8', 'sample_intermediate_size_9', 'sample_intermediate_size_10', 'sample_intermediate_size_11', 'sample_num_attention_heads_0', 'sample_num_attention_heads_1', 'sample_num_attention_heads_2', 'sample_num_attention_heads_3', 'sample_num_attention_heads_4', 'sample_num_attention_heads_5', 'sample_num_attention_heads_6', 'sample_num_attention_heads_7', 'sample_num_attention_heads_8', 'sample_num_attention_heads_9', 'sample_num_attention_heads_10', 'sample_num_attention_heads_11']
Subnet info: elastickey2ranges= {'sample_hidden_size': [0, 1], 'sample_intermediate_size': [1, 13], 'sample_num_attention_heads': [13, 25]}
pre-initialized with BERT weights
perplexity before starting: 7.00
/data/home/ganayu/miniconda/envs/basic/lib/python3.8/site-packages/transformers/configuration_utils.py:336: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.
  warnings.warn(
08/21/2022 01:35:11 - INFO - __main__ -   ==================================================================
08/21/2022 01:35:11 - INFO - __main__ -   Number of parameters in custom config is 67 Million
08/21/2022 01:35:11 - INFO - __main__ -   ==================================================================
08/21/2022 01:35:19 - INFO - __main__ -   =====================================================================================
Layer (type:depth-idx)                                       Param #
=====================================================================================
BertForMaskedLM                                              --
├─BertModel: 1-1                                             --
│    └─BertEmbeddings: 2-1                                   --
│    │    └─CustomEmbedding: 3-1                             23,440,896
│    │    └─CustomEmbedding: 3-2                             393,216
│    │    └─CustomEmbedding: 3-3                             1,536
│    │    └─CustomLayerNorm: 3-4                             1,536
│    │    └─Dropout: 3-5                                     --
│    └─BertEncoder: 2-2                                      --
│    │    └─ModuleList: 3-6                                  --
│    │    │    └─BertLayer: 4-1                              7,087,872
│    │    │    └─BertLayer: 4-2                              7,087,872
│    │    │    └─BertLayer: 4-3                              7,087,872
│    │    │    └─BertLayer: 4-4                              7,087,872
│    │    │    └─BertLayer: 4-5                              7,087,872
│    │    │    └─BertLayer: 4-6                              7,087,872
│    │    │    └─BertLayer: 4-7                              7,087,872
│    │    │    └─BertLayer: 4-8                              7,087,872
│    │    │    └─BertLayer: 4-9                              7,087,872
│    │    │    └─BertLayer: 4-10                             7,087,872
│    │    │    └─BertLayer: 4-11                             7,087,872
│    │    │    └─BertLayer: 4-12                             7,087,872
├─BertOnlyMLMHead: 1-2                                       --
│    └─BertLMPredictionHead: 2-3                             --
│    │    └─BertPredictionHeadTransform: 3-7                 --
│    │    │    └─CustomLinear: 4-13                          590,592
│    │    │    └─CustomLayerNorm: 4-14                       1,536
│    │    └─CustomLinear: 3-8                                23,471,418
=====================================================================================
Total params: 109,514,298
Trainable params: 109,514,298
Non-trainable params: 0
=====================================================================================
08/21/2022 01:35:19 - INFO - __main__ -   Skipping tokenization! as we have the tokenized dataset is already loaded from /fsx/ganayu/data/academic_bert_dataset/final_bert_preproc_128
08/21/2022 01:35:19 - INFO - __main__ -   Sample 7471301 of the training set: {'input_ids': [101, 4448, 2003, 1037, 2883, 1011, 4351, 2173, 1006, 8561, 1007, 1999, 16222, 9626, 3600, 2221, 1010, 3448, 1012, 2004, 1997, 1996, 2230, 2883, 1010, 2049, 2313, 2001, 6564, 2475, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.
08/21/2022 01:35:19 - INFO - __main__ -   Sample 1678443 of the training set: {'input_ids': [101, 1012, 2009, 2790, 2008, 2016, 2018, 1037, 3167, 8406, 1999, 2010, 3437, 1012, 1000, 2331, 2003, 8552, 1012, 2111, 2006, 3011, 7166, 2000, 2228, 2008, 2331, 2003, 2345, 1010, 2030, 2008, 1037, 2117, 2166, 3310, 2044, 2331, 1012, 1045, 2123, 1005, 1056, 2156, 2331, 1999, 2593, 1997, 2216, 3971, 1012, 2108, 2757, 2003, 2074, 2178, 2126, 2008, 2057, 4671, 2054, 2057, 2024, 2081, 1997, 1012, 1996, 7209, 2126, 1997, 6595, 4142, 1999, 1996, 3151, 3168, 2003, 2000, 3413, 2006, 2256, 9165, 2011, 2383, 2336, 1012, 2008, 1005, 1055, 2339, 2057, 3280, 1010, 4312, 1012, 1000, 1000, 2054, 2079, 2017, 2812, 1029, 1000, 1000, 1996, 3114, 2057, 3280, 2003, 2138, 2057, 3413, 2006, 2256, 9165, 1010, 2000, 2191, 2488, 4617, 1997, 9731, 2612, 1997, 29486, 2075, 2256, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.
08/21/2022 01:35:19 - INFO - __main__ -   Sample 18456905 of the training set: {'input_ids': [101, 2006, 2538, 2233, 2889, 1010, 1996, 8773, 2864, 2049, 10494, 3462, 1012, 2206, 2006, 2013, 2049, 2034, 3462, 1010, 2009, 2506, 2000, 2022, 2109, 2005, 3231, 7599, 2005, 2070, 2051, 1010, 14313, 2119, 1996, 16736, 1997, 1996, 4145, 1998, 1997, 1996, 2047, 24264, 2072, 1011, 6377, 3001, 1012, 2096, 1996, 10003, 7160, 2064, 4232, 9563, 1997, 1996, 2250, 15643, 2018, 2042, 6025, 1010, 5604, 2001, 4208, 2588, 1996, 2047, 20704, 3258, 6558, 5361, 1010, 2029, 2020, 2056, 2011, 24264, 2072, 2000, 2191, 2005, 1037, 4659, 2715, 4959, 2948, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.
08/21/2022 01:35:25 - INFO - __main__ -   Number of steps/updates per epoch: 72436
08/21/2022 01:35:25 - INFO - __main__ -   ***** Running training *****
08/21/2022 01:35:25 - INFO - __main__ -     Num examples = 37087091
08/21/2022 01:35:25 - INFO - __main__ -     Num Epochs = 2
08/21/2022 01:35:25 - INFO - __main__ -     Instantaneous batch size per device = 128
08/21/2022 01:35:25 - INFO - __main__ -     Total train batch size (w. parallel, distributed & accumulation) = 512
08/21/2022 01:35:25 - INFO - __main__ -     Gradient Accumulation steps = 2
08/21/2022 01:35:25 - INFO - __main__ -     Total optimization steps = 125000, 0 steps completed so far
  0%|                                                                                                              | 0/125000 [00:00<?, ?it/s]08/21/2022 01:35:25 - INFO - __main__ -   =============================
08/21/2022 01:35:25 - INFO - __main__ -   Starting training from epoch 0
08/21/2022 01:35:25 - INFO - __main__ -   Training till epoch  2
08/21/2022 01:35:25 - INFO - __main__ -   =============================
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 177/177 [01:06<00:00,  2.66it/s]
08/21/2022 01:36:32 - INFO - __main__ -   perplexity before starting: 7.00 █████████████████████████████████| 177/177 [01:06<00:00,  3.25it/s]
Traceback (most recent call last):
  File "train_mlm.py", line 2230, in <module>
    main()
  File "train_mlm.py", line 1745, in main
    for step, batch in enumerate(train_dataloader):
  File "/data/home/ganayu/miniconda/envs/basic/lib/python3.8/site-packages/accelerate/data_loader.py", line 301, in __iter__
    for batch in super().__iter__():
  File "/data/home/ganayu/miniconda/envs/basic/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 368, in __iter__
    return self._get_iterator()
  File "/data/home/ganayu/miniconda/envs/basic/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 314, in _get_iterator
    return _MultiProcessingDataLoaderIter(self)
  File "/data/home/ganayu/miniconda/envs/basic/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 965, in __init__
    self._reset(loader, first_iter=True)
  File "/data/home/ganayu/miniconda/envs/basic/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 996, in _reset
    self._try_put_index()
  File "/data/home/ganayu/miniconda/envs/basic/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 1230, in _try_put_index
    index = self._next_index()
  File "/data/home/ganayu/miniconda/envs/basic/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 521, in _next_index
    return next(self._sampler_iter)  # may raise StopIteration
  File "/data/home/ganayu/miniconda/envs/basic/lib/python3.8/site-packages/accelerate/data_loader.py", line 149, in _iter_with_no_split
    for idx, batch in enumerate(self.batch_sampler):
  File "/data/home/ganayu/miniconda/envs/basic/lib/python3.8/site-packages/torch/utils/data/sampler.py", line 226, in __iter__
    for idx in self.sampler:
  File "/data/home/ganayu/miniconda/envs/basic/lib/python3.8/site-packages/torch/utils/data/sampler.py", line 122, in __iter__
    yield from torch.randperm(n, generator=generator).tolist()
KeyboardInterrupt