/data/home/ganayu/miniconda/envs/basic/lib/python3.8/site-packages/transformers/configuration_utils.py:336: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.
  warnings.warn(
07/25/2022 00:40:56 - INFO - __main__ -   MobileBert Initiliazed with bert-base
07/25/2022 00:40:56 - INFO - __main__ -   =====================================================================================
Layer (type:depth-idx)                                       Param #
=====================================================================================
MobileBertForMaskedLM                                        --
├─MobileBertModel: 1-1                                       --
│    └─BertEmbeddings: 2-1                                   --
│    │    └─CustomEmbedding: 3-1                             23,440,896
│    │    └─CustomEmbedding: 3-2                             393,216
│    │    └─CustomEmbedding: 3-3                             1,536
│    │    └─CustomLayerNorm: 3-4                             1,536
│    │    └─Dropout: 3-5                                     --
│    └─MobileBertEncoder: 2-2                                --
│    │    └─ModuleList: 3-6                                  --
│    │    │    └─MobileBertLayer: 4-1                        8,272,896
│    │    │    └─MobileBertLayer: 4-2                        8,272,896
│    │    │    └─MobileBertLayer: 4-3                        8,272,896
│    │    │    └─MobileBertLayer: 4-4                        8,272,896
│    │    │    └─MobileBertLayer: 4-5                        8,272,896
│    │    │    └─MobileBertLayer: 4-6                        8,272,896
│    │    │    └─MobileBertLayer: 4-7                        8,272,896
│    │    │    └─MobileBertLayer: 4-8                        8,272,896
│    │    │    └─MobileBertLayer: 4-9                        8,272,896
│    │    │    └─MobileBertLayer: 4-10                       8,272,896
│    │    │    └─MobileBertLayer: 4-11                       8,272,896
│    │    │    └─MobileBertLayer: 4-12                       8,272,896
├─MobileBertOnlyMLMHead: 1-2                                 --
│    └─MobileBertLMPredictionHead: 2-3                       --
│    │    └─MobileBertPredictionHeadTransform: 3-7           --
│    │    │    └─CustomLinear: 4-13                          590,592
│    │    │    └─CustomLayerNorm: 4-14                       1,536
│    │    └─CustomLinear: 3-8                                23,471,418
=====================================================================================
Total params: 123,734,586
Trainable params: 123,734,586
Non-trainable params: 0
=====================================================================================
07/25/2022 00:40:57 - INFO - __main__ -   Skipping tokenization! as we have the tokenized dataset is already loaded from /fsx/ganayu/data/bert_pretraining_data/wikibooks_graphcore_128_next_sentence_label_removed_w_splits
07/25/2022 00:40:57 - INFO - __main__ -   Sample 10727801 of the training set: {'input_ids': [101, 8799, 2649, 2014, 2679, 2004, 1037, 1000, 9535, 3723, 2836, 1000, 1998, 2158, 12662, 2050, 3090, 1010, 1000, 2016, 2743, 2428, 2204, 1010, 2074, 2117, 2190, 1012, 1000, 102, 2119, 13989, 1998, 10365, 5228, 4847, 2005, 1996, 2836, 1997, 1996, 3453, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.
07/25/2022 00:40:57 - INFO - __main__ -   Sample 1867825 of the training set: {'input_ids': [101, 8362, 2000, 1996, 4746, 1999, 1996, 2262, 2621, 3783, 1010, 2174, 1010, 2931, 7576, 2020, 2583, 2000, 5566, 1999, 2035, 1996, 2168, 2998, 2004, 2273, 1012, 1999, 1996, 3467, 3783, 1010, 2308, 2024, 2145, 4039, 2000, 5566, 1999, 1996, 13649, 4117, 1012, 2045, 2024, 2747, 2048, 4386, 2824, 1999, 2029, 3287, 7576, 2089, 2025, 5566, 1024, 26351, 8093, 27296, 2098, 5742, 1998, 14797, 102, 2093, 25225, 2015, 2000, 3413, 2302, 1037, 7401, 1997, 1996, 2399, 1024, 1996, 4947, 2399, 2020, 8014, 2138, 1997, 2088, 2162, 1045, 1010, 1998, 1996, 2621, 1998, 3467, 2399, 1997, 3878, 1998, 3646, 2020, 8014, 2138, 1997, 2088, 2162, 2462, 1012, 1996, 17023, 1011, 9166, 2162, 2090, 4108, 1998, 3607, 12591, 2006, 1996, 3098, 2154, 1997, 1996, 2263, 2621, 3783, 1999, 7211, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}.
07/25/2022 00:40:57 - INFO - __main__ -   Sample 419610 of the training set: {'input_ids': [101, 5115, 2005, 2570, 1516, 2603, 2244, 2325, 1012, 999, 8902, 13102, 2319, 1027, 1000, 1017, 1000, 2806, 1027, 1000, 4281, 1024, 1001, 10507, 9468, 4246, 1025, 1000, 1064, 2570, 2244, 999, 8902, 13102, 2319, 1027, 1000, 1017, 1000, 2806, 1027, 1000, 4281, 1024, 1001, 10507, 9468, 4246, 1025, 1000, 1064, 2603, 2244, 2461, 1016, 1012, 1996, 2117, 2461, 7208, 2024, 5115, 2005, 1021, 1516, 2403, 2255, 2325, 1012, 999, 8902, 13102, 2319, 1027, 1000, 1017, 1000, 2806, 1027, 1000, 4281, 1024, 102, 999, 8902, 13102, 2319, 1027, 1000, 1017, 1000, 2806, 1027, 1000, 4281, 1024, 1001, 10507, 9468, 4246, 1025, 1000, 1064, 1022, 2255, 999, 8902, 13102, 2319, 1027, 1000, 1017, 1000, 2806, 1027, 1000, 4281, 1024, 1001, 10507, 9468, 4246, 1025, 1000, 1064, 1023, 2255, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}.
Traceback (most recent call last):
  File "train_mlm.py", line 2032, in <module>
    main()
  File "train_mlm.py", line 1370, in main
    model.set_sample_config(global_config, drop_layers=False)
TypeError: set_sample_config() got an unexpected keyword argument 'drop_layers'