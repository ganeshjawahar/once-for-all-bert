
pre-initialized with BERT weights
/data/home/ganayu/miniconda/envs/basic/lib/python3.8/site-packages/transformers/configuration_utils.py:336: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.
  warnings.warn(
08/28/2022 05:51:57 - INFO - __main__ -   BERT-Bottleneck Initiliazed with BERT-base
08/28/2022 05:51:57 - INFO - __main__ -   Other experts Initiliazed with BERT-base