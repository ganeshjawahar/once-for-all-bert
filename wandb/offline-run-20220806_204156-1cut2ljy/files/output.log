Subnet info: Model-Size=60580308, Val-PPL=4.888035877106968
Subnet info: Gene=[540, 540, 540, 540, 768, 360, 540, 540, 768, 540, 768, 768, 3, 2, 4, 3, 3, 4, 3, 4, 4, 3, 4, 4, 9]
Subnet info: Search_space_id=v2
Subnet info: elastic_keys= ['sample_hidden_size', 'sample_intermediate_size', 'sample_num_hidden_layers']
Subnet info: gene_choices= [[120, 360, 540, 768], [120, 360, 540, 768], [120, 360, 540, 768], [120, 360, 540, 768], [120, 360, 540, 768], [120, 360, 540, 768], [120, 360, 540, 768], [120, 360, 540, 768], [120, 360, 540, 768], [120, 360, 540, 768], [120, 360, 540, 768], [120, 360, 540, 768], [2, 3, 4], [2, 3, 4], [2, 3, 4], [2, 3, 4], [2, 3, 4], [2, 3, 4], [2, 3, 4], [2, 3, 4], [2, 3, 4], [2, 3, 4], [2, 3, 4], [2, 3, 4], [6, 9, 12]]
Subnet info: gene_names= ['sample_hidden_size_0', 'sample_hidden_size_1', 'sample_hidden_size_2', 'sample_hidden_size_3', 'sample_hidden_size_4', 'sample_hidden_size_5', 'sample_hidden_size_6', 'sample_hidden_size_7', 'sample_hidden_size_8', 'sample_hidden_size_9', 'sample_hidden_size_10', 'sample_hidden_size_11', 'sample_intermediate_size_0', 'sample_intermediate_size_1', 'sample_intermediate_size_2', 'sample_intermediate_size_3', 'sample_intermediate_size_4', 'sample_intermediate_size_5', 'sample_intermediate_size_6', 'sample_intermediate_size_7', 'sample_intermediate_size_8', 'sample_intermediate_size_9', 'sample_intermediate_size_10', 'sample_intermediate_size_11', 'sample_num_hidden_layers_0']
Subnet info: elastickey2ranges= {'sample_hidden_size': [0, 12], 'sample_intermediate_size': [12, 24], 'sample_num_hidden_layers': [24, 25]}
pre-initialized with BERT weights
/data/home/ganayu/miniconda/envs/basic/lib/python3.8/site-packages/transformers/configuration_utils.py:336: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.
  warnings.warn(
08/06/2022 20:42:02 - INFO - __main__ -   ==================================================================
08/06/2022 20:42:02 - INFO - __main__ -   Number of parameters in custom config is 61 Million
08/06/2022 20:42:02 - INFO - __main__ -   ==================================================================
08/06/2022 20:42:09 - INFO - __main__ -   BERT-Bottleneck Initiliazed with BERT-base
08/06/2022 20:42:09 - INFO - __main__ -   =====================================================================================
Layer (type:depth-idx)                                       Param #
=====================================================================================
BertForMaskedLM                                              --
├─BertModel: 1-1                                             --
│    └─BertEmbeddings: 2-1                                   --
│    │    └─CustomEmbedding: 3-1                             23,440,896
│    │    └─CustomEmbedding: 3-2                             393,216
│    │    └─CustomEmbedding: 3-3                             1,536
│    │    └─CustomLayerNorm: 3-4                             1,536
│    │    └─Dropout: 3-5                                     --
│    └─BertEncoder: 2-2                                      --
│    │    └─ModuleList: 3-6                                  --
│    │    │    └─BertLayer: 4-1                              8,269,056
│    │    │    └─BertLayer: 4-2                              8,269,056
│    │    │    └─BertLayer: 4-3                              8,269,056
│    │    │    └─BertLayer: 4-4                              8,269,056
│    │    │    └─BertLayer: 4-5                              8,269,056
│    │    │    └─BertLayer: 4-6                              8,269,056
│    │    │    └─BertLayer: 4-7                              8,269,056
│    │    │    └─BertLayer: 4-8                              8,269,056
│    │    │    └─BertLayer: 4-9                              8,269,056
├─BertOnlyMLMHead: 1-2                                       --
│    └─BertLMPredictionHead: 2-3                             --
│    │    └─BertPredictionHeadTransform: 3-7                 --
│    │    │    └─CustomLinear: 4-10                          590,592
│    │    │    └─CustomLayerNorm: 4-11                       1,536
│    │    └─CustomLinear: 3-8                                23,471,418
=====================================================================================
Total params: 98,881,338
Trainable params: 98,881,338
Non-trainable params: 0
=====================================================================================
08/06/2022 20:42:09 - INFO - __main__ -   Skipping tokenization! as we have the tokenized dataset is already loaded from /fsx/ganayu/data/academic_bert_dataset/bert_preproc_128
08/06/2022 20:42:09 - INFO - __main__ -   Sample 5363900 of the training set: {'input_ids': [101, 1005, 1055, 3100, 1012, 2009, 1005, 1055, 1037, 2204, 3104, 1012, 2065, 1045, 1005, 1049, 2183, 2000, 2022, 2725, 3313, 4611, 1010, 2731, 2007, 2017, 4364, 1998, 2652, 2007, 1996, 2316, 1010, 2045, 1005, 2222, 2022, 2625, 3980, 2055, 2026, 6134, 2065, 2027, 2228, 2057, 1005, 2128, 5306, 1012, 4661, 1010, 2027, 2525, 2031, 2158, 1011, 10188, 2229, 2006, 2017, 1010, 2044, 1996, 2126, 2017, 8971, 6838, 1012, 1000, 1000, 2027, 1005, 2128, 2025, 2428, 2026, 2828, 1010, 1000, 2002, 14217, 1012, 1000, 2562, 2115, 2159, 2006, 1996, 2346, 1010, 12390, 1010, 1000, 1045, 8040, 11614, 2098, 2007, 1037, 5861, 1012, 1000, 3398, 1010, 3398, 1010, 3100, 1012, 2055, 2008, 3124, 1010, 6838, 1012, 2002, 8572, 2017, 2411, 1029, 1000, 1000, 2053, 1010, 2025, 2428, 1012, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.
08/06/2022 20:42:09 - INFO - __main__ -   Sample 933912 of the training set: {'input_ids': [101, 2068, 2012, 2256, 12257, 1005, 1012, 1005, 2017, 2113, 1005, 1010, 17438, 2056, 5399, 25636, 14547, 1012, 1005, 2122, 4286, 2031, 2048, 7047, 2000, 7324, 2023, 3291, 2006, 1996, 2132, 1012, 2028, 5724, 2052, 2022, 2000, 3423, 5562, 2054, 2024, 2747, 25049, 13978, 1012, 2174, 1025, 2074, 2004, 2038, 2042, 2464, 2007, 6544, 1025, 1996, 2529, 2554, 3475, 1005, 1056, 3625, 1998, 2969, 28675, 2438, 2000, 5047, 2009, 1012, 2065, 9009, 1998, 4847, 2005, 2500, 1025, 2008, 2003, 2074, 2004, 2057, 2031, 1999, 2256, 2451, 1025, 2071, 2022, 16021, 28345, 2098, 1999, 2529, 2554, 2023, 5724, 2052, 2763, 2147, 1012, 2023, 3727, 1996, 2117, 1998, 2069, 5724, 1025, 1998, 2008, 2003, 2000, 2191, 2122, 13978, 6206, 1012, 2174, 4286, 2064, 1005, 1056, 2130, 2131, 2023, 2157, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.
08/06/2022 20:42:09 - INFO - __main__ -   Sample 209805 of the training set: {'input_ids': [101, 2000, 2224, 2026, 4435, 1011, 1999, 1011, 5456, 1010, 3422, 3233, 11839, 1010, 2046, 1037, 4132, 2005, 2023, 2338, 1998, 2049, 3260, 1012, 2002, 2245, 1996, 2338, 2453, 2022, 6179, 1010, 2029, 2001, 2035, 1045, 2734, 2000, 2131, 2000, 2147, 1012, 1006, 2005, 1996, 2501, 1010, 2002, 12999, 2033, 2025, 2000, 2139, 16467, 2023, 2338, 2000, 2032, 2138, 1997, 2010, 10218, 14910, 15148, 1012, 2021, 1010, 2002, 5621, 4427, 1996, 19143, 1997, 2116, 14981, 1045, 2071, 2025, 7532, 1012, 2302, 2010, 14469, 16718, 2055, 2010, 2219, 2166, 1010, 2023, 2338, 2052, 2025, 2031, 2042, 2517, 1012, 2009, 2052, 2022, 1037, 4126, 1997, 26324, 2025, 2000, 2139, 16467, 2009, 2000, 2032, 1012, 2061, 1010, 2164, 5207, 1005, 1055, 2171, 2001, 2349, 4847, 2000, 3071, 2144, 2002, 2036, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.
08/06/2022 20:42:17 - INFO - __main__ -   Number of steps/updates per epoch: 12678
08/06/2022 20:42:17 - INFO - __main__ -   ***** Running training *****
08/06/2022 20:42:17 - INFO - __main__ -     Num examples = 6491001
08/06/2022 20:42:17 - INFO - __main__ -     Num Epochs = 8
08/06/2022 20:42:17 - INFO - __main__ -     Instantaneous batch size per device = 128
08/06/2022 20:42:17 - INFO - __main__ -     Total train batch size (w. parallel, distributed & accumulation) = 512
08/06/2022 20:42:17 - INFO - __main__ -     Gradient Accumulation steps = 2
08/06/2022 20:42:17 - INFO - __main__ -     Total optimization steps = 100000, 0 steps completed so far
  0%|                                                                                                           | 0/100000 [00:00<?, ?it/s]08/06/2022 20:42:17 - INFO - __main__ -   =============================
08/06/2022 20:42:17 - INFO - __main__ -   Starting training from epoch 0
08/06/2022 20:42:17 - INFO - __main__ -   Training till epoch  8
08/06/2022 20:42:17 - INFO - __main__ -   =============================
  0%|                                                                                                    | 1/902 [00:06<1:41:45,  6.78s/it]
  0%|                                                                                                           | 0/100000 [00:07<?, ?it/s]