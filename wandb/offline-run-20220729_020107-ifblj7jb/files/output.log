pre-initialized with BERT weights
[768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768]
/data/home/ganayu/miniconda/envs/basic/lib/python3.8/site-packages/transformers/configuration_utils.py:336: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.
  warnings.warn(
07/29/2022 02:01:30 - INFO - __main__ -   BERT-Bottleneck Initiliazed with BERT-base
07/29/2022 02:01:31 - INFO - __main__ -   =====================================================================================
Layer (type:depth-idx)                                       Param #
=====================================================================================
BertForMaskedLM                                              --
├─BertModel: 1-1                                             --
│    └─BertEmbeddings: 2-1                                   --
│    │    └─CustomEmbedding: 3-1                             23,440,896
│    │    └─CustomEmbedding: 3-2                             393,216
│    │    └─CustomEmbedding: 3-3                             1,536
│    │    └─CustomLayerNorm: 3-4                             1,536
│    │    └─Dropout: 3-5                                     --
│    └─BertEncoder: 2-2                                      --
│    │    └─ModuleList: 3-6                                  --
│    │    │    └─BertLayer: 4-1                              13,339,520
│    │    │    └─BertLayer: 4-2                              13,339,520
│    │    │    └─BertLayer: 4-3                              13,339,520
│    │    │    └─BertLayer: 4-4                              13,339,520
│    │    │    └─BertLayer: 4-5                              13,339,520
│    │    │    └─BertLayer: 4-6                              13,339,520
│    │    │    └─BertLayer: 4-7                              13,339,520
│    │    │    └─BertLayer: 4-8                              13,339,520
│    │    │    └─BertLayer: 4-9                              13,339,520
│    │    │    └─BertLayer: 4-10                             13,339,520
│    │    │    └─BertLayer: 4-11                             13,339,520
│    │    │    └─BertLayer: 4-12                             13,339,520
├─BertOnlyMLMHead: 1-2                                       --
│    └─BertLMPredictionHead: 2-3                             --
│    │    └─BertPredictionHeadTransform: 3-7                 --
│    │    │    └─CustomLinear: 4-13                          590,592
│    │    │    └─CustomLayerNorm: 4-14                       1,536
│    │    └─CustomLinear: 3-8                                23,471,418
=====================================================================================
Total params: 184,534,074
Trainable params: 184,534,074
Non-trainable params: 0
=====================================================================================
07/29/2022 02:01:31 - INFO - __main__ -   Skipping tokenization! as we have the tokenized dataset is already loaded from /fsx/ganayu/data/bert_pretraining_data/wikibooks_graphcore_128_next_sentence_label_removed_w_splits
07/29/2022 02:01:31 - INFO - __main__ -   Sample 10727801 of the training set: {'input_ids': [101, 8799, 2649, 2014, 2679, 2004, 1037, 1000, 9535, 3723, 2836, 1000, 1998, 2158, 12662, 2050, 3090, 1010, 1000, 2016, 2743, 2428, 2204, 1010, 2074, 2117, 2190, 1012, 1000, 102, 2119, 13989, 1998, 10365, 5228, 4847, 2005, 1996, 2836, 1997, 1996, 3453, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.
07/29/2022 02:01:31 - INFO - __main__ -   Sample 1867825 of the training set: {'input_ids': [101, 8362, 2000, 1996, 4746, 1999, 1996, 2262, 2621, 3783, 1010, 2174, 1010, 2931, 7576, 2020, 2583, 2000, 5566, 1999, 2035, 1996, 2168, 2998, 2004, 2273, 1012, 1999, 1996, 3467, 3783, 1010, 2308, 2024, 2145, 4039, 2000, 5566, 1999, 1996, 13649, 4117, 1012, 2045, 2024, 2747, 2048, 4386, 2824, 1999, 2029, 3287, 7576, 2089, 2025, 5566, 1024, 26351, 8093, 27296, 2098, 5742, 1998, 14797, 102, 2093, 25225, 2015, 2000, 3413, 2302, 1037, 7401, 1997, 1996, 2399, 1024, 1996, 4947, 2399, 2020, 8014, 2138, 1997, 2088, 2162, 1045, 1010, 1998, 1996, 2621, 1998, 3467, 2399, 1997, 3878, 1998, 3646, 2020, 8014, 2138, 1997, 2088, 2162, 2462, 1012, 1996, 17023, 1011, 9166, 2162, 2090, 4108, 1998, 3607, 12591, 2006, 1996, 3098, 2154, 1997, 1996, 2263, 2621, 3783, 1999, 7211, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}.
07/29/2022 02:01:31 - INFO - __main__ -   Sample 419610 of the training set: {'input_ids': [101, 5115, 2005, 2570, 1516, 2603, 2244, 2325, 1012, 999, 8902, 13102, 2319, 1027, 1000, 1017, 1000, 2806, 1027, 1000, 4281, 1024, 1001, 10507, 9468, 4246, 1025, 1000, 1064, 2570, 2244, 999, 8902, 13102, 2319, 1027, 1000, 1017, 1000, 2806, 1027, 1000, 4281, 1024, 1001, 10507, 9468, 4246, 1025, 1000, 1064, 2603, 2244, 2461, 1016, 1012, 1996, 2117, 2461, 7208, 2024, 5115, 2005, 1021, 1516, 2403, 2255, 2325, 1012, 999, 8902, 13102, 2319, 1027, 1000, 1017, 1000, 2806, 1027, 1000, 4281, 1024, 102, 999, 8902, 13102, 2319, 1027, 1000, 1017, 1000, 2806, 1027, 1000, 4281, 1024, 1001, 10507, 9468, 4246, 1025, 1000, 1064, 1022, 2255, 999, 8902, 13102, 2319, 1027, 1000, 1017, 1000, 2806, 1027, 1000, 4281, 1024, 1001, 10507, 9468, 4246, 1025, 1000, 1064, 1023, 2255, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}.
Traceback (most recent call last):
  File "train_mlm.py", line 2088, in <module>
    main()
  File "train_mlm.py", line 1407, in main
    model.set_sample_config(global_config, drop_layers=False)
  File "/fsx/ganayu/code/SuperShaper/custom_layers/custom_bert.py", line 2526, in set_sample_config
    self.bert.set_sample_config(
  File "/fsx/ganayu/code/SuperShaper/custom_layers/custom_bert.py", line 2021, in set_sample_config
    self.encoder.set_sample_config(
  File "/fsx/ganayu/code/SuperShaper/custom_layers/custom_bert.py", line 1563, in set_sample_config
    layer.set_sample_config(layer_config, is_identity_layer=False)
  File "/fsx/ganayu/code/SuperShaper/custom_layers/custom_bert.py", line 1340, in set_sample_config
    print(min_max_normalization(config.master_sample_hidden_size))
  File "/fsx/ganayu/code/SuperShaper/custom_layers/custom_bert.py", line 126, in min_max_normalization
    arr[i] = (arr[i] - min_val)/(max_val - min_val)
ZeroDivisionError: float division by zero