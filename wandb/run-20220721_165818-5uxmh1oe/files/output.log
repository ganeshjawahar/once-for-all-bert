15145643 1598708 84142
BertConfig {
  "additional_random_softmaxing": false,
  "alpha_divergence": 0,
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_drop_prob": 0.0,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "max_seq_length": 128,
  "mixing": "attention",
  "model_type": "bert",
  "normalization_type": "layer_norm",
  "num_attention_heads": 12,
  "num_feedforward_networks": 1,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "random_layer_selection_probability": 0.1,
  "rewire": 0,
  "sample_hidden_size": 768,
  "sample_intermediate_size": [
    3072,
    3072,
    3072,
    3072,
    3072,
    3072,
    3072,
    3072,
    3072,
    3072,
    3072,
    3072
  ],
  "sample_num_attention_heads": [
    12,
    12,
    12,
    12,
    12,
    12,
    12,
    12,
    12,
    12,
    12,
    12
  ],
  "sample_num_hidden_layers": 12,
  "transformers_version": "4.11.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}
initialized with random BERT weights
/data/home/ganayu/miniconda/envs/basic/lib/python3.8/site-packages/transformers/configuration_utils.py:336: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.
  warnings.warn(
/data/home/ganayu/miniconda/envs/basic/lib/python3.8/site-packages/transformers/configuration_utils.py:336: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.
  warnings.warn(
07/21/2022 16:58:27 - INFO - __main__ -   ==================================================================
07/21/2022 16:58:27 - INFO - __main__ -   Number of parameters in custom config is 117 Million
07/21/2022 16:58:27 - INFO - __main__ -   ==================================================================
07/21/2022 16:58:37 - INFO - __main__ -   =====================================================================================
Layer (type:depth-idx)                                       Param #
=====================================================================================
BertForMaskedLM                                              --
├─BertModel: 1-1                                             --
│    └─BertEmbeddings: 2-1                                   --
│    │    └─CustomEmbedding: 3-1                             23,440,896
│    │    └─CustomEmbedding: 3-2                             393,216
│    │    └─CustomEmbedding: 3-3                             1,536
│    │    └─CustomLayerNorm: 3-4                             1,536
│    │    └─Dropout: 3-5                                     --
│    └─BertEncoder: 2-2                                      --
│    │    └─ModuleList: 3-6                                  --
│    │    │    └─BertLayer: 4-1                              7,087,872
│    │    │    └─BertLayer: 4-2                              7,087,872
│    │    │    └─BertLayer: 4-3                              7,087,872
│    │    │    └─BertLayer: 4-4                              7,087,872
│    │    │    └─BertLayer: 4-5                              7,087,872
│    │    │    └─BertLayer: 4-6                              7,087,872
│    │    │    └─BertLayer: 4-7                              7,087,872
│    │    │    └─BertLayer: 4-8                              7,087,872
│    │    │    └─BertLayer: 4-9                              7,087,872
│    │    │    └─BertLayer: 4-10                             7,087,872
│    │    │    └─BertLayer: 4-11                             7,087,872
│    │    │    └─BertLayer: 4-12                             7,087,872
├─BertOnlyMLMHead: 1-2                                       --
│    └─BertLMPredictionHead: 2-3                             --
│    │    └─BertPredictionHeadTransform: 3-7                 --
│    │    │    └─CustomLinear: 4-13                          590,592
│    │    │    └─CustomLayerNorm: 4-14                       1,536
│    │    └─CustomLinear: 3-8                                23,471,418
=====================================================================================
Total params: 109,514,298
Trainable params: 109,514,298
Non-trainable params: 0
=====================================================================================
07/21/2022 16:58:37 - INFO - __main__ -   Skipping tokenization! as we have the tokenized dataset is already loaded from /fsx/ganayu/data/bert_pretraining_data/wikibooks_graphcore_128_next_sentence_label_removed_w_splits
07/21/2022 16:58:37 - INFO - __main__ -   Sample 10727801 of the training set: {'input_ids': [101, 8799, 2649, 2014, 2679, 2004, 1037, 1000, 9535, 3723, 2836, 1000, 1998, 2158, 12662, 2050, 3090, 1010, 1000, 2016, 2743, 2428, 2204, 1010, 2074, 2117, 2190, 1012, 1000, 102, 2119, 13989, 1998, 10365, 5228, 4847, 2005, 1996, 2836, 1997, 1996, 3453, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.
07/21/2022 16:58:37 - INFO - __main__ -   Sample 1867825 of the training set: {'input_ids': [101, 8362, 2000, 1996, 4746, 1999, 1996, 2262, 2621, 3783, 1010, 2174, 1010, 2931, 7576, 2020, 2583, 2000, 5566, 1999, 2035, 1996, 2168, 2998, 2004, 2273, 1012, 1999, 1996, 3467, 3783, 1010, 2308, 2024, 2145, 4039, 2000, 5566, 1999, 1996, 13649, 4117, 1012, 2045, 2024, 2747, 2048, 4386, 2824, 1999, 2029, 3287, 7576, 2089, 2025, 5566, 1024, 26351, 8093, 27296, 2098, 5742, 1998, 14797, 102, 2093, 25225, 2015, 2000, 3413, 2302, 1037, 7401, 1997, 1996, 2399, 1024, 1996, 4947, 2399, 2020, 8014, 2138, 1997, 2088, 2162, 1045, 1010, 1998, 1996, 2621, 1998, 3467, 2399, 1997, 3878, 1998, 3646, 2020, 8014, 2138, 1997, 2088, 2162, 2462, 1012, 1996, 17023, 1011, 9166, 2162, 2090, 4108, 1998, 3607, 12591, 2006, 1996, 3098, 2154, 1997, 1996, 2263, 2621, 3783, 1999, 7211, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}.
07/21/2022 16:58:37 - INFO - __main__ -   Sample 419610 of the training set: {'input_ids': [101, 5115, 2005, 2570, 1516, 2603, 2244, 2325, 1012, 999, 8902, 13102, 2319, 1027, 1000, 1017, 1000, 2806, 1027, 1000, 4281, 1024, 1001, 10507, 9468, 4246, 1025, 1000, 1064, 2570, 2244, 999, 8902, 13102, 2319, 1027, 1000, 1017, 1000, 2806, 1027, 1000, 4281, 1024, 1001, 10507, 9468, 4246, 1025, 1000, 1064, 2603, 2244, 2461, 1016, 1012, 1996, 2117, 2461, 7208, 2024, 5115, 2005, 1021, 1516, 2403, 2255, 2325, 1012, 999, 8902, 13102, 2319, 1027, 1000, 1017, 1000, 2806, 1027, 1000, 4281, 1024, 102, 999, 8902, 13102, 2319, 1027, 1000, 1017, 1000, 2806, 1027, 1000, 4281, 1024, 1001, 10507, 9468, 4246, 1025, 1000, 1064, 1022, 2255, 999, 8902, 13102, 2319, 1027, 1000, 1017, 1000, 2806, 1027, 1000, 4281, 1024, 1001, 10507, 9468, 4246, 1025, 1000, 1064, 1023, 2255, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}.
07/21/2022 16:58:42 - INFO - __main__ -   Number of steps/updates per epoch: 29582
07/21/2022 16:58:42 - INFO - __main__ -   ***** Running training *****
07/21/2022 16:58:42 - INFO - __main__ -     Num examples = 15145643
07/21/2022 16:58:42 - INFO - __main__ -     Num Epochs = 5
07/21/2022 16:58:42 - INFO - __main__ -     Instantaneous batch size per device = 16
07/21/2022 16:58:42 - INFO - __main__ -     Total train batch size (w. parallel, distributed & accumulation) = 512
07/21/2022 16:58:42 - INFO - __main__ -     Gradient Accumulation steps = 16
07/21/2022 16:58:42 - INFO - __main__ -     Total optimization steps = 125000, 0 steps completed so far
  0%|                                                                                                                               | 0/125000 [00:00<?, ?it/s]07/21/2022 16:58:42 - INFO - __main__ -   =============================
07/21/2022 16:58:42 - INFO - __main__ -   Starting training from epoch 0
07/21/2022 16:58:42 - INFO - __main__ -   Training till epoch  5
07/21/2022 16:58:42 - INFO - __main__ -   =============================















































  File "train_mlm.py", line 1959, in <module>▏                                                                              | 889/2630 [01:37<03:10,  9.15it/s]
    main()
  File "train_mlm.py", line 1472, in main
    eval_metric = validate_subtransformer(
  File "train_mlm.py", line 133, in validate_subtransformer
    outputs = model(**batch)
  File "/data/home/ganayu/miniconda/envs/basic/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/fsx/ganayu/code/SuperShaper/utils/module_proxy_wrapper.py", line 53, in forward
    return self.module(*args, **kwargs)
  File "/data/home/ganayu/miniconda/envs/basic/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/data/home/ganayu/miniconda/envs/basic/lib/python3.8/site-packages/accelerate/utils/operations.py", line 487, in __call__
    return convert_to_fp32(self.model_forward(*args, **kwargs))
  File "/data/home/ganayu/miniconda/envs/basic/lib/python3.8/site-packages/torch/autocast_mode.py", line 12, in decorate_autocast
    return func(*args, **kwargs)
  File "/data/home/ganayu/miniconda/envs/basic/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 963, in forward
    output = self.module(*inputs[0], **kwargs[0])
  File "/data/home/ganayu/miniconda/envs/basic/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/fsx/ganayu/code/SuperShaper/custom_layers/custom_bert.py", line 2551, in forward
    outputs = self.bert(
  File "/data/home/ganayu/miniconda/envs/basic/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/fsx/ganayu/code/SuperShaper/custom_layers/custom_bert.py", line 2130, in forward
    encoder_outputs = self.encoder(
  File "/data/home/ganayu/miniconda/envs/basic/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/fsx/ganayu/code/SuperShaper/custom_layers/custom_bert.py", line 1622, in forward
    layer_outputs = layer_module(
  File "/data/home/ganayu/miniconda/envs/basic/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/fsx/ganayu/code/SuperShaper/custom_layers/custom_bert.py", line 1430, in forward
    layer_output = apply_chunking_to_forward(
  File "/data/home/ganayu/miniconda/envs/basic/lib/python3.8/site-packages/transformers/modeling_utils.py", line 2299, in apply_chunking_to_forward
    num_args_in_forward_chunk_fn = len(inspect.signature(forward_fn).parameters)
  File "/data/home/ganayu/miniconda/envs/basic/lib/python3.8/inspect.py", line 3105, in signature
    return Signature.from_callable(obj, follow_wrapped=follow_wrapped)
  File "/data/home/ganayu/miniconda/envs/basic/lib/python3.8/inspect.py", line 2854, in from_callable
    return _signature_from_callable(obj, sigcls=cls,
  File "/data/home/ganayu/miniconda/envs/basic/lib/python3.8/inspect.py", line 2233, in _signature_from_callable
    sig = _signature_from_callable(
  File "/data/home/ganayu/miniconda/envs/basic/lib/python3.8/inspect.py", line 2304, in _signature_from_callable
    return _signature_from_function(sigcls, obj,
  File "/data/home/ganayu/miniconda/envs/basic/lib/python3.8/inspect.py", line 2147, in _signature_from_function
    positional = arg_names[:pos_count]
KeyboardInterrupt