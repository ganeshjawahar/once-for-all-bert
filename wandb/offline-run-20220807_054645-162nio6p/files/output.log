
pre-initialized with BERT weights
/data/home/ganayu/miniconda/envs/basic/lib/python3.8/site-packages/transformers/configuration_utils.py:336: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.
  warnings.warn(
Traceback (most recent call last):
  File "train_mlm.py", line 2146, in <module>
    main()
  File "train_mlm.py", line 1187, in main
    model = custom_bert.BertForMaskedLM.from_pretrained(args.model_name_or_path, config=global_config)
  File "/data/home/ganayu/miniconda/envs/basic/lib/python3.8/site-packages/transformers/modeling_utils.py", line 1385, in from_pretrained
    model = cls(config, *model_args, **model_kwargs)
  File "/fsx/ganayu/code/SuperShaper/custom_layers/custom_bert.py", line 2518, in __init__
  File "/fsx/ganayu/code/SuperShaper/custom_layers/custom_bert.py", line 2007, in __init__
    self.config = config
  File "/fsx/ganayu/code/SuperShaper/custom_layers/custom_bert.py", line 1525, in __init__
    self.use_bottleneck = config.mixing == "bert-bottleneck"
  File "/fsx/ganayu/code/SuperShaper/custom_layers/custom_bert.py", line 1525, in <listcomp>
    self.use_bottleneck = config.mixing == "bert-bottleneck"
  File "/fsx/ganayu/code/SuperShaper/custom_layers/custom_bert.py", line 1339, in __init__
    ), f"{self} should be used as a decoder model if cross attention is added"
NameError: name 'BertConv1dI' is not defined