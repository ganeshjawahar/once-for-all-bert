{
    "os": "Linux-5.13.0-1025-aws-x86_64-with-glibc2.17",
    "python": "3.8.13",
    "heartbeatAt": "2022-07-16T23:01:37.179992",
    "startedAt": "2022-07-16T23:01:34.790684",
    "docker": null,
    "gpu": "NVIDIA A100-SXM4-40GB",
    "gpu_count": 2,
    "cpu_count": 96,
    "cuda": "11.0.228",
    "args": [
        "--per_device_train_batch_size",
        "16",
        "--per_device_eval_batch_size",
        "16",
        "--gradient_accumulation_steps",
        "16",
        "--fp16",
        "1",
        "--max_seq_length",
        "512",
        "--mixing",
        "bert-bottleneck",
        "--max_train_steps",
        "125000",
        "--tokenized_c4_dir",
        "/fsx/ganayu/data/bert_pretraining_data/wikibooks_datasets_dummy_tokenized",
        "--model_name_or_path",
        "bert-base-cased",
        "--sampling_type",
        "none",
        "--sampling_rule",
        "none",
        "--learning_rate",
        "0.0001",
        "--weight_decay",
        "0.01",
        "--num_warmup_steps",
        "10000",
        "--eval_random_subtransformers",
        "0",
        "--output_dir",
        "/tmp/",
        "--preprocessing_num_workers",
        "1",
        "--betas_2",
        "0.98",
        "--subtransformer_config_path",
        "/fsx/ganayu/experiments/supershaper/configs/bert/bertbase_12L_120H.csv",
        "--wandb_suffix",
        "12L_768H",
        "--wandb_entity",
        "ganayu",
        "--wandb_project",
        "effbert"
    ],
    "state": "running",
    "program": "train_mlm.py",
    "codePath": "train_mlm.py",
    "git": {
        "remote": "https://github.com/ganeshjawahar/once-for-all-bert.git",
        "commit": "c66cd174b29ead4f03495e277f296ed883e5a8f2"
    },
    "email": null,
    "root": "/fsx/ganayu/code/SuperShaper",
    "host": "a100-st-p4d24xlarge-4",
    "username": "ganayu",
    "executable": "/data/home/ganayu/miniconda/envs/basic/bin/python"
}
