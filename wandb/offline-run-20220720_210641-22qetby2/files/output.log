task labels =  ['not_equivalent', 'equivalent']
BertConfig {
  "additional_random_softmaxing": false,
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_drop_prob": 0.0,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "max_seq_length": 128,
  "mixing": "bert-bottleneck",
  "model_type": "bert",
  "normalization_type": "layer_norm",
  "num_attention_heads": 12,
  "num_feedforward_networks": 1,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "random_layer_selection_probability": 0.1,
  "rewire": 0,
  "sample_hidden_size": [
    120,
    120,
    120,
    120,
    120,
    120,
    120,
    120,
    120,
    120,
    120,
    120
  ],
  "sample_intermediate_size": [
    3072,
    3072,
    3072,
    3072,
    3072,
    3072,
    3072,
    3072,
    3072,
    3072,
    3072,
    3072
  ],
  "sample_num_attention_heads": [
    12,
    12,
    12,
    12,
    12,
    12,
    12,
    12,
    12,
    12,
    12,
    12
  ],
  "sample_num_hidden_layers": 12,
  "transformers_version": "4.11.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}
mrpc epoch 0 best accuracy: 0.697265625
mrpc epoch 1 best accuracy: 0.697265625
mrpc epoch 2 best accuracy: 0.697265625
mrpc epoch 3 best accuracy: 0.703125
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 373.77it/s]
/data/home/ganayu/miniconda/envs/basic/lib/python3.8/site-packages/transformers/configuration_utils.py:336: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.
  warnings.warn(
/data/home/ganayu/miniconda/envs/basic/lib/python3.8/site-packages/transformers/configuration_utils.py:336: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.
  warnings.warn(
07/20/2022 21:06:47 - INFO - __main__ -   ==================================================================
07/20/2022 21:06:47 - INFO - __main__ -   Number of parameters in custom config is 34 Million
07/20/2022 21:06:47 - INFO - __main__ -   ==================================================================
Some weights of the model checkpoint at /fsx/ganayu/experiments/supershaper/jul19_v3_bertstandalone/12L_120H/epoch_13 were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /fsx/ganayu/experiments/supershaper/jul19_v3_bertstandalone/12L_120H/epoch_13 and are newly initialized: ['classifier.weight', 'bert.pooler.dense.bias', 'classifier.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
07/20/2022 21:06:49 - INFO - __main__ -   =====================================================================================
Layer (type:depth-idx)                                       Param #
=====================================================================================
BertForSequenceClassification                                --
├─BertModel: 1-1                                             --
│    └─BertEmbeddings: 2-1                                   --
│    │    └─CustomEmbedding: 3-1                             23,440,896
│    │    └─CustomEmbedding: 3-2                             393,216
│    │    └─CustomEmbedding: 3-3                             1,536
│    │    └─CustomLayerNorm: 3-4                             1,536
│    │    └─Dropout: 3-5                                     --
│    └─BertEncoder: 2-2                                      --
│    │    └─ModuleList: 3-6                                  --
│    │    │    └─BertLayer: 4-1                              8,269,056
│    │    │    └─BertLayer: 4-2                              8,269,056
│    │    │    └─BertLayer: 4-3                              8,269,056
│    │    │    └─BertLayer: 4-4                              8,269,056
│    │    │    └─BertLayer: 4-5                              8,269,056
│    │    │    └─BertLayer: 4-6                              8,269,056
│    │    │    └─BertLayer: 4-7                              8,269,056
│    │    │    └─BertLayer: 4-8                              8,269,056
│    │    │    └─BertLayer: 4-9                              8,269,056
│    │    │    └─BertLayer: 4-10                             8,269,056
│    │    │    └─BertLayer: 4-11                             8,269,056
│    │    │    └─BertLayer: 4-12                             8,269,056
│    └─BertPooler: 2-3                                       --
│    │    └─CustomLinear: 3-7                                590,592
│    │    └─Tanh: 3-8                                        --
├─Dropout: 1-2                                               --
├─CustomLinear: 1-3                                          1,538
=====================================================================================
Total params: 123,657,986
Trainable params: 123,657,986
Non-trainable params: 0
=====================================================================================
07/20/2022 21:06:49 - INFO - __main__ -   Label2Id:  None
07/20/2022 21:06:49 - INFO - __main__ -   Sample 1126 of the training set: {'input_ids': [101, 107, 2563, 1708, 18874, 1144, 10887, 4441, 1115, 1103, 3000, 4612, 1104, 2563, 1708, 18874, 1105, 19265, 4876, 6432, 12638, 15308, 1105, 170, 2418, 17843, 1115, 1103, 13618, 1156, 1129, 11018, 119, 107, 102, 2563, 1708, 18874, 1144, 4491, 1115, 1126, 19265, 17748, 1156, 1339, 6432, 12638, 15308, 1105, 170, 2418, 17843, 1115, 1103, 13618, 1156, 1129, 11018, 119, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': 1}.
07/20/2022 21:06:49 - INFO - __main__ -   Sample 1003 of the training set: {'input_ids': [101, 2907, 9170, 117, 1103, 5883, 12649, 112, 188, 2313, 1809, 1565, 1207, 6435, 118, 1210, 1664, 118, 2380, 6683, 1105, 1210, 2380, 6683, 119, 102, 2907, 9170, 117, 1103, 5883, 12649, 112, 188, 2313, 1809, 1565, 1207, 6435, 1106, 1103, 2313, 1105, 1231, 118, 1809, 1565, 1639, 119, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': 0}.
07/20/2022 21:06:49 - INFO - __main__ -   Sample 914 of the training set: {'input_ids': [101, 107, 1284, 1294, 1185, 170, 27632, 1111, 4006, 1451, 2732, 1236, 1936, 1106, 3244, 1103, 1237, 1470, 1121, 1748, 9640, 2035, 117, 107, 5934, 3291, 4206, 15960, 1163, 119, 102, 107, 1284, 1294, 1185, 170, 27632, 1111, 4006, 1451, 2732, 1236, 1936, 1106, 3244, 1103, 1237, 1470, 1121, 1748, 9640, 3690, 117, 107, 1163, 5934, 3291, 4206, 15960, 117, 8603, 12012, 112, 188, 3181, 4848, 119, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': 1}.
07/20/2022 21:06:53 - INFO - __main__ -   ***** Running training *****
07/20/2022 21:06:53 - INFO - __main__ -     Num examples = 3668
07/20/2022 21:06:53 - INFO - __main__ -     Num Epochs = 10
07/20/2022 21:06:53 - INFO - __main__ -     Instantaneous batch size per device = 16
07/20/2022 21:06:53 - INFO - __main__ -     Total train batch size (w. parallel, distributed & accumulation) = 32
07/20/2022 21:06:53 - INFO - __main__ -     Gradient Accumulation steps = 1
07/20/2022 21:06:53 - INFO - __main__ -     Total optimization steps = 1150, 0 steps completed so far
  0%|                                                                                                           | 0/1150 [00:00<?, ?it/s]07/20/2022 21:06:53 - INFO - __main__ -   =============================
07/20/2022 21:06:53 - INFO - __main__ -   Starting training from epoch 0
07/20/2022 21:06:53 - INFO - __main__ -   Training till epoch  10
07/20/2022 21:06:53 - INFO - __main__ -   =============================
[W reducer.cpp:1289] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
 10%|█████████▋                                                                                       | 115/1150 [00:09<01:19, 13.03it/s]07/20/2022 21:07:05 - INFO - __main__ -   epoch 0: {'accuracy': 0.697265625, 'f1': 0.8148148148148148}
/data/home/ganayu/miniconda/envs/basic/lib/python3.8/site-packages/transformers/configuration_utils.py:336: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.
  warnings.warn(
 20%|███████████████████▎                                                                             | 229/1150 [00:24<01:20, 11.41it/s]07/20/2022 21:07:20 - INFO - __main__ -   epoch 1: {'accuracy': 0.689453125, 'f1': 0.7800829875518672}
 30%|█████████████████████████████                                                                    | 345/1150 [00:36<01:08, 11.69it/s]07/20/2022 21:07:31 - INFO - __main__ -   epoch 2: {'accuracy': 0.693359375, 'f1': 0.7920529801324504}
 40%|██████████████████████████████████████▋                                                          | 459/1150 [00:46<00:51, 13.32it/s]07/20/2022 21:07:42 - INFO - __main__ -   epoch 3: {'accuracy': 0.703125, 'f1': 0.8132678132678132}
/data/home/ganayu/miniconda/envs/basic/lib/python3.8/site-packages/transformers/configuration_utils.py:336: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.
  warnings.warn(
 47%|█████████████████████████████████████████████▉                                                   | 545/1150 [00:59<00:53, 11.23it/s]Error in sys.excepthook:
Traceback (most recent call last):
  File "/data/home/ganayu/miniconda/envs/basic/lib/python3.8/site-packages/wandb/sdk/lib/exit_hooks.py", line 54, in exc_handler
    traceback.print_exception(exc_type, exc, tb)
  File "/data/home/ganayu/miniconda/envs/basic/lib/python3.8/traceback.py", line 103, in print_exception
    for line in TracebackException(
  File "/data/home/ganayu/miniconda/envs/basic/lib/python3.8/traceback.py", line 508, in __init__
    self.stack = StackSummary.extract(
  File "/data/home/ganayu/miniconda/envs/basic/lib/python3.8/traceback.py", line 353, in extract
    linecache.lazycache(filename, f.f_globals)
  File "/data/home/ganayu/miniconda/envs/basic/lib/python3.8/linecache.py", line 171, in lazycache
    get_source = getattr(loader, 'get_source', None)
KeyboardInterrupt
Original exception was:
Traceback (most recent call last):
  File "train_glue.py", line 1246, in <module>
    main()
  File "train_glue.py", line 1151, in main
    outputs = model(**batch)
  File "/data/home/ganayu/miniconda/envs/basic/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/fsx/ganayu/code/SuperShaper/utils/module_proxy_wrapper.py", line 53, in forward
    return self.module(*args, **kwargs)
  File "/data/home/ganayu/miniconda/envs/basic/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/data/home/ganayu/miniconda/envs/basic/lib/python3.8/site-packages/accelerate/utils/operations.py", line 487, in __call__
    return convert_to_fp32(self.model_forward(*args, **kwargs))
  File "/data/home/ganayu/miniconda/envs/basic/lib/python3.8/site-packages/torch/autocast_mode.py", line 12, in decorate_autocast
    return func(*args, **kwargs)
  File "/data/home/ganayu/miniconda/envs/basic/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 963, in forward
    output = self.module(*inputs[0], **kwargs[0])
  File "/data/home/ganayu/miniconda/envs/basic/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/fsx/ganayu/code/SuperShaper/custom_layers/custom_bert.py", line 2833, in forward
    outputs = self.bert(
  File "/data/home/ganayu/miniconda/envs/basic/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/fsx/ganayu/code/SuperShaper/custom_layers/custom_bert.py", line 2130, in forward
    encoder_outputs = self.encoder(
  File "/data/home/ganayu/miniconda/envs/basic/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/fsx/ganayu/code/SuperShaper/custom_layers/custom_bert.py", line 1622, in forward
    layer_outputs = layer_module(
  File "/data/home/ganayu/miniconda/envs/basic/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/fsx/ganayu/code/SuperShaper/custom_layers/custom_bert.py", line 1384, in forward
    self_attention_outputs = self.attention(
  File "/data/home/ganayu/miniconda/envs/basic/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/fsx/ganayu/code/SuperShaper/custom_layers/custom_bert.py", line 1195, in forward
    attention_output = self.output(self_outputs[0], hidden_states)
  File "/data/home/ganayu/miniconda/envs/basic/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/fsx/ganayu/code/SuperShaper/custom_layers/custom_bert.py", line 587, in forward
    hidden_states = self.LayerNorm(hidden_states + input_tensor)
  File "/data/home/ganayu/miniconda/envs/basic/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/fsx/ganayu/code/SuperShaper/custom_layers/custom_layernorm.py", line 56, in forward
    return F.layer_norm(
  File "/data/home/ganayu/miniconda/envs/basic/lib/python3.8/site-packages/torch/nn/functional.py", line 2482, in layer_norm
    if has_torch_function_variadic(input, weight, bias):
KeyboardInterrupt