accelerate launch --config_file /fsx/ganayu/code/SuperShaper/default_config.yaml --main_process_port 20625 train_mlm.py --per_device_train_batch_size 128 --per_device_eval_batch_size 256 --gradient_accumulation_steps 2 --fp16 1 --max_seq_length 128 --mixing attention --max_train_steps 125000 --tokenized_c4_dir /fsx/ganayu/data/bert_pretraining_data/wikibooks_graphcore_128_next_sentence_label_removed_w_splits --model_name_or_path bert-base-uncased --sampling_type none --sampling_rule none --learning_rate 0.0005 --weight_decay 0.01 --num_warmup_steps 10000 --eval_random_subtransformers 0 --output_dir /tmp --preprocessing_num_workers 1 --betas_2 0.98 --tokenizer_name bert-base-uncased --subtransformer_config_path /fsx/ganayu/experiments/supershaper/configs/bert/bertbase_uncased_nobottleneck_12L_768H.csv --experiment_name jul21_v3_google-bert --check_test_loss_only yes --check_test_loss_only_percent 0.1 --wandb_suffix google-bert --wandb_entity ganayu --wandb_project effbert
accelerate launch --config_file /fsx/ganayu/code/SuperShaper/default_config.yaml --main_process_port 20615 train_mlm.py --per_device_train_batch_size 128 --per_device_eval_batch_size 256 --gradient_accumulation_steps 2 --fp16 1 --max_seq_length 128 --mixing attention --max_train_steps 125000 --tokenized_c4_dir /fsx/ganayu/data/bert_pretraining_data/wikibooks_graphcore_128_next_sentence_label_removed_w_splits --model_name_or_path Graphcore/bert-base-uncased --sampling_type none --sampling_rule none --learning_rate 0.0005 --weight_decay 0.01 --num_warmup_steps 10000 --eval_random_subtransformers 0 --output_dir /tmp --preprocessing_num_workers 1 --betas_2 0.98 --tokenizer_name Graphcore/bert-base-uncased --subtransformer_config_path /fsx/ganayu/experiments/supershaper/configs/bert/bertbase_uncased_nobottleneck_12L_768H.csv --experiment_name jul21_v3_google-bert --check_test_loss_only yes --check_test_loss_only_percent 0.1 --wandb_suffix google-bert --wandb_entity ganayu --wandb_project effbert
accelerate launch --config_file /fsx/ganayu/code/SuperShaper/default_config.yaml --main_process_port 20605 train_mlm.py --per_device_train_batch_size 16 --per_device_eval_batch_size 256 --gradient_accumulation_steps 16 --fp16 1 --max_seq_length 128 --mixing bert-bottleneck --max_train_steps 125000 --tokenized_c4_dir /fsx/ganayu/data/bert_pretraining_data/wikibooks_graphcore_128_next_sentence_label_removed_w_splits --model_name_or_path /fsx/ganayu/experiments/supershaper/jul19_v3_bertstandalone/12L_768H/best_model --sampling_type none --sampling_rule none --learning_rate 0.0001 --weight_decay 0.01 --num_warmup_steps 10000 --eval_random_subtransformers 0 --output_dir /tmp/ --preprocessing_num_workers 1 --betas_2 0.98 --subtransformer_config_path /fsx/ganayu/experiments/supershaper/configs/bert/bertbase_uncased_12L_768H.csv --wandb_suffix 12L_768H --wandb_entity ganayu --wandb_project effbert --tokenizer_name Graphcore/bert-base-uncased --check_test_loss_only yes --check_test_loss_only_percent 0.1